{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Naive Bayes Spam Classification"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:30:05.561514Z",
     "start_time": "2025-10-10T01:30:04.445428Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Collection\n",
    "Load the labeled email dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:30:54.128056Z",
     "start_time": "2025-10-10T01:30:54.101118Z"
    }
   },
   "source": [
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "df = df[['v1', 'v2']]\n",
    "df.columns = ['label', 'message']\n",
    "\n",
    "print(f\"Total emails in dataset: {len(df)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nFirst few emails:\")\n",
    "df.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emails in dataset: 5572\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First few emails:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Extraction\n",
    "Process text to extract words as features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:32:23.610110Z",
     "start_time": "2025-10-10T01:32:23.595453Z"
    }
   },
   "source": [
    "# Separate spam and ham emails\n",
    "spam_emails = df[df['label'] == 'spam']['message'].tolist()\n",
    "ham_emails = df[df['label'] == 'ham']['message'].tolist()\n",
    "\n",
    "print(f\"Spam emails: {len(spam_emails)}\")\n",
    "print(f\"Ham emails: {len(ham_emails)}\")\n",
    "print(f\"\\nExample spam email: {spam_emails[0]}\")\n",
    "print(f\"\\nExample ham email: {ham_emails[0]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam emails: 747\n",
      "Ham emails: 4825\n",
      "\n",
      "Example spam email: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "Example ham email: Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training Data Preparation\n",
    "Count word occurrences in each class"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:34:42.202846Z",
     "start_time": "2025-10-10T01:34:42.171901Z"
    }
   },
   "source": [
    "# Initialize word count dictionaries and vocabulary set\n",
    "spam_word_count = defaultdict(int)\n",
    "ham_word_count = defaultdict(int)\n",
    "vocabulary = set()\n",
    "\n",
    "# Process spam emails\n",
    "for email in spam_emails:\n",
    "    text = email.lower()\n",
    "    # Remove single letters and numbers\n",
    "    text = re.sub(r'\\b[a-z]\\b|\\d+', '', text)\n",
    "    # Remove non-alphabetic characters except spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    # Filter words with length > 1\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "    \n",
    "    for word in words:\n",
    "        spam_word_count[word] += 1\n",
    "        vocabulary.add(word)\n",
    "\n",
    "print(f\"Total words in spam emails: {sum(spam_word_count.values())}\")\n",
    "print(f\"Unique words in spam: {len(spam_word_count)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in spam emails: 14880\n",
      "Unique words in spam: 2215\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:34:42.621777Z",
     "start_time": "2025-10-10T01:34:42.509229Z"
    }
   },
   "source": [
    "# Process ham emails\n",
    "for email in ham_emails:\n",
    "    text = email.lower()\n",
    "    # Remove single letters and numbers\n",
    "    text = re.sub(r'\\b[a-z]\\b|\\d+', '', text)\n",
    "    # Remove non-alphabetic characters except spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    # Filter words with length > 1\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "    \n",
    "    for word in words:\n",
    "        ham_word_count[word] += 1\n",
    "        vocabulary.add(word)\n",
    "\n",
    "print(f\"Total words in ham emails: {sum(ham_word_count.values())}\")\n",
    "print(f\"Unique words in ham: {len(ham_word_count)}\")\n",
    "print(f\"\\nTotal vocabulary size: {len(vocabulary)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in ham emails: 61011\n",
      "Unique words in ham: 7118\n",
      "\n",
      "Total vocabulary size: 8321\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Calculating Probabilities\n",
    "### a) Prior Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:35:41.110395Z",
     "start_time": "2025-10-10T01:35:41.098691Z"
    }
   },
   "source": [
    "# Calculate prior probabilities\n",
    "total_emails = len(spam_emails) + len(ham_emails)\n",
    "prior_spam = len(spam_emails) / total_emails\n",
    "prior_ham = len(ham_emails) / total_emails\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"PRIOR PROBABILITIES\")\n",
    "print(\"=\"*50)\n",
    "print(f\"P(Spam) = {len(spam_emails)}/{total_emails} = {prior_spam:.4f}\")\n",
    "print(f\"P(Ham) = {len(ham_emails)}/{total_emails} = {prior_ham:.4f}\")\n",
    "print(\"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PRIOR PROBABILITIES\n",
      "==================================================\n",
      "P(Spam) = 747/5572 = 0.1341\n",
      "P(Ham) = 4825/5572 = 0.8659\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Likelihood Probabilities with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:35:46.917634Z",
     "start_time": "2025-10-10T01:35:46.910812Z"
    }
   },
   "source": [
    "# Calculate total word counts\n",
    "total_spam_words = sum(spam_word_count.values())\n",
    "total_ham_words = sum(ham_word_count.values())\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "print(f\"Total words in spam: {total_spam_words}\")\n",
    "print(f\"Total words in ham: {total_ham_words}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in spam: 14880\n",
      "Total words in ham: 61011\n",
      "Vocabulary size: 8321\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:36:01.372482Z",
     "start_time": "2025-10-10T01:36:01.362070Z"
    }
   },
   "source": [
    "# Example: Calculate likelihood for some common spam words\n",
    "example_words = ['free', 'win', 'click', 'now', 'buy']\n",
    "\n",
    "print(\"\\nExample Likelihood Probabilities (with Laplace smoothing):\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Word':<15} {'P(word|Spam)':<20} {'P(word|Ham)':<20}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for word in example_words:\n",
    "    # With Laplace smoothing: (count + 1) / (total_words + vocab_size)\n",
    "    spam_count = spam_word_count.get(word, 0)\n",
    "    ham_count = ham_word_count.get(word, 0)\n",
    "    \n",
    "    p_word_spam = (spam_count + 1) / (total_spam_words + vocab_size)\n",
    "    p_word_ham = (ham_count + 1) / (total_ham_words + vocab_size)\n",
    "    \n",
    "    print(f\"{word:<15} {p_word_spam:<20.8f} {p_word_ham:<20.8f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Likelihood Probabilities (with Laplace smoothing):\n",
      "======================================================================\n",
      "Word            P(word|Spam)         P(word|Ham)         \n",
      "======================================================================\n",
      "free            0.00948235           0.00086540          \n",
      "win             0.00267230           0.00017308          \n",
      "click           0.00025861           0.00004327          \n",
      "now             0.00823240           0.00421162          \n",
      "buy             0.00017241           0.00090867          \n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Storing the Model\n",
    "Store all the trained parameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:38:22.190796Z",
     "start_time": "2025-10-10T01:38:22.180551Z"
    }
   },
   "source": [
    "# Store model parameters\n",
    "model = {\n",
    "    'prior_spam': prior_spam,\n",
    "    'prior_ham': prior_ham,\n",
    "    'spam_word_count': spam_word_count,\n",
    "    'ham_word_count': ham_word_count,\n",
    "    'vocabulary': vocabulary,\n",
    "    'total_spam_words': total_spam_words,\n",
    "    'total_ham_words': total_ham_words,\n",
    "    'vocab_size': vocab_size\n",
    "}\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"MODEL TRAINED AND STORED SUCCESSFULLY!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Prior P(Spam): {prior_spam:.4f}\")\n",
    "print(f\"Prior P(Ham): {prior_ham:.4f}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Total spam words: {total_spam_words}\")\n",
    "print(f\"Total ham words: {total_ham_words}\")\n",
    "print(\"=\"*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "MODEL TRAINED AND STORED SUCCESSFULLY!\n",
      "==================================================\n",
      "Prior P(Spam): 0.1341\n",
      "Prior P(Ham): 0.8659\n",
      "Vocabulary size: 8321\n",
      "Total spam words: 14880\n",
      "Total ham words: 61011\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Classification\n",
    "Classify new emails using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:38:26.702143Z",
     "start_time": "2025-10-10T01:38:26.688975Z"
    }
   },
   "source": [
    "# Test Email 1: Obvious spam\n",
    "test_email_1 = \"Congratulations! You've won a free iPhone! Click here now!\"\n",
    "\n",
    "print(\"Testing Email 1:\")\n",
    "print(f\"Message: {test_email_1}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Process the email\n",
    "text = test_email_1.lower()\n",
    "text = re.sub(r'\\b[a-z]\\b|\\d+', '', text)\n",
    "text = re.sub(r'[^a-z\\s]', '', text)\n",
    "words = text.split()\n",
    "words = [word for word in words if len(word) > 1]\n",
    "\n",
    "print(f\"Extracted words: {words}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate log probabilities (to avoid underflow)\n",
    "log_prob_spam = np.log(prior_spam)\n",
    "log_prob_ham = np.log(prior_ham)\n",
    "\n",
    "print(f\"\\nStarting with priors:\")\n",
    "print(f\"log P(Spam) = {log_prob_spam:.4f}\")\n",
    "print(f\"log P(Ham) = {log_prob_ham:.4f}\")\n",
    "print(\"\\nMultiplying by word likelihoods:\")\n",
    "\n",
    "for word in words:\n",
    "    # Calculate P(word|Spam) with Laplace smoothing\n",
    "    spam_count = spam_word_count.get(word, 0)\n",
    "    p_word_spam = (spam_count + 1) / (total_spam_words + vocab_size)\n",
    "    \n",
    "    # Calculate P(word|Ham) with Laplace smoothing\n",
    "    ham_count = ham_word_count.get(word, 0)\n",
    "    p_word_ham = (ham_count + 1) / (total_ham_words + vocab_size)\n",
    "    \n",
    "    log_prob_spam += np.log(p_word_spam)\n",
    "    log_prob_ham += np.log(p_word_ham)\n",
    "    \n",
    "    print(f\"  {word}: P(word|Spam)={p_word_spam:.6f}, P(word|Ham)={p_word_ham:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Final log P(Spam|email) = {log_prob_spam:.4f}\")\n",
    "print(f\"Final log P(Ham|email) = {log_prob_ham:.4f}\")\n",
    "\n",
    "if log_prob_spam > log_prob_ham:\n",
    "    classification = \"SPAM\"\n",
    "else:\n",
    "    classification = \"HAM\"\n",
    "\n",
    "print(f\"\\n>>> Classification: {classification} <<<\")\n",
    "print(\"=\"*70)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Email 1:\n",
      "Message: Congratulations! You've won a free iPhone! Click here now!\n",
      "\n",
      "======================================================================\n",
      "Extracted words: ['congratulations', 'youve', 'won', 'free', 'iphone', 'click', 'here', 'now']\n",
      "======================================================================\n",
      "\n",
      "Starting with priors:\n",
      "log P(Spam) = -2.0094\n",
      "log P(Ham) = -0.1439\n",
      "\n",
      "Multiplying by word likelihoods:\n",
      "  congratulations: P(word|Spam)=0.000647, P(word|Ham)=0.000029\n",
      "  youve: P(word|Spam)=0.000216, P(word|Ham)=0.000159\n",
      "  won: P(word|Spam)=0.003319, P(word|Ham)=0.000288\n",
      "  free: P(word|Spam)=0.009482, P(word|Ham)=0.000865\n",
      "  iphone: P(word|Spam)=0.000043, P(word|Ham)=0.000029\n",
      "  click: P(word|Spam)=0.000259, P(word|Ham)=0.000043\n",
      "  here: P(word|Spam)=0.000302, P(word|Ham)=0.001615\n",
      "  now: P(word|Spam)=0.008232, P(word|Ham)=0.004212\n",
      "\n",
      "======================================================================\n",
      "Final log P(Spam|email) = -59.3802\n",
      "Final log P(Ham|email) = -66.9491\n",
      "\n",
      ">>> Classification: SPAM <<<\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:38:26.989340Z",
     "start_time": "2025-10-10T01:38:26.973611Z"
    }
   },
   "source": [
    "# Test Email 2: Legitimate message\n",
    "test_email_2 = \"Hey, do you want to meet for coffee tomorrow afternoon?\"\n",
    "\n",
    "print(\"Testing Email 2:\")\n",
    "print(f\"Message: {test_email_2}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Process the email\n",
    "text = test_email_2.lower()\n",
    "text = re.sub(r'\\b[a-z]\\b|\\d+', '', text)\n",
    "text = re.sub(r'[^a-z\\s]', '', text)\n",
    "words = text.split()\n",
    "words = [word for word in words if len(word) > 1]\n",
    "\n",
    "print(f\"Extracted words: {words}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate log probabilities\n",
    "log_prob_spam = np.log(prior_spam)\n",
    "log_prob_ham = np.log(prior_ham)\n",
    "\n",
    "print(f\"\\nStarting with priors:\")\n",
    "print(f\"log P(Spam) = {log_prob_spam:.4f}\")\n",
    "print(f\"log P(Ham) = {log_prob_ham:.4f}\")\n",
    "print(\"\\nMultiplying by word likelihoods:\")\n",
    "\n",
    "for word in words:\n",
    "    # Calculate P(word|Spam) with Laplace smoothing\n",
    "    spam_count = spam_word_count.get(word, 0)\n",
    "    p_word_spam = (spam_count + 1) / (total_spam_words + vocab_size)\n",
    "    \n",
    "    # Calculate P(word|Ham) with Laplace smoothing\n",
    "    ham_count = ham_word_count.get(word, 0)\n",
    "    p_word_ham = (ham_count + 1) / (total_ham_words + vocab_size)\n",
    "    \n",
    "    log_prob_spam += np.log(p_word_spam)\n",
    "    log_prob_ham += np.log(p_word_ham)\n",
    "    \n",
    "    print(f\"  {word}: P(word|Spam)={p_word_spam:.6f}, P(word|Ham)={p_word_ham:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Final log P(Spam|email) = {log_prob_spam:.4f}\")\n",
    "print(f\"Final log P(Ham|email) = {log_prob_ham:.4f}\")\n",
    "\n",
    "if log_prob_spam > log_prob_ham:\n",
    "    classification = \"SPAM\"\n",
    "else:\n",
    "    classification = \"HAM\"\n",
    "\n",
    "print(f\"\\n>>> Classification: {classification} <<<\")\n",
    "print(\"=\"*70)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Email 2:\n",
      "Message: Hey, do you want to meet for coffee tomorrow afternoon?\n",
      "\n",
      "======================================================================\n",
      "Extracted words: ['hey', 'do', 'you', 'want', 'to', 'meet', 'for', 'coffee', 'tomorrow', 'afternoon']\n",
      "======================================================================\n",
      "\n",
      "Starting with priors:\n",
      "log P(Spam) = -2.0094\n",
      "log P(Ham) = -0.1439\n",
      "\n",
      "Multiplying by word likelihoods:\n",
      "  hey: P(word|Spam)=0.000259, P(word|Ham)=0.001543\n",
      "  do: P(word|Spam)=0.001034, P(word|Ham)=0.005466\n",
      "  you: P(word|Spam)=0.012413, P(word|Ham)=0.026640\n",
      "  want: P(word|Spam)=0.001293, P(word|Ham)=0.002365\n",
      "  to: P(word|Spam)=0.029611, P(word|Ham)=0.022428\n",
      "  meet: P(word|Spam)=0.000302, P(word|Ham)=0.001053\n",
      "  for: P(word|Spam)=0.008836, P(word|Ham)=0.007241\n",
      "  coffee: P(word|Spam)=0.000043, P(word|Ham)=0.000115\n",
      "  tomorrow: P(word|Spam)=0.000474, P(word|Ham)=0.001168\n",
      "  afternoon: P(word|Spam)=0.000043, P(word|Ham)=0.000404\n",
      "\n",
      "======================================================================\n",
      "Final log P(Spam|email) = -72.2958\n",
      "Final log P(Ham|email) = -60.7146\n",
      "\n",
      ">>> Classification: HAM <<<\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:38:27.440571Z",
     "start_time": "2025-10-10T01:38:27.429023Z"
    }
   },
   "source": [
    "# Test Email 3: Another example\n",
    "test_email_3 = \"Quick cash opportunity\"\n",
    "\n",
    "print(\"Testing Email 3:\")\n",
    "print(f\"Message: {test_email_3}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Process the email\n",
    "text = test_email_3.lower()\n",
    "text = re.sub(r'\\b[a-z]\\b|\\d+', '', text)\n",
    "text = re.sub(r'[^a-z\\s]', '', text)\n",
    "words = text.split()\n",
    "words = [word for word in words if len(word) > 1]\n",
    "\n",
    "print(f\"Extracted words: {words}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate log probabilities\n",
    "log_prob_spam = np.log(prior_spam)\n",
    "log_prob_ham = np.log(prior_ham)\n",
    "\n",
    "print(f\"\\nStarting with priors:\")\n",
    "print(f\"log P(Spam) = {log_prob_spam:.4f}\")\n",
    "print(f\"log P(Ham) = {log_prob_ham:.4f}\")\n",
    "print(\"\\nMultiplying by word likelihoods:\")\n",
    "\n",
    "for word in words:\n",
    "    spam_count = spam_word_count.get(word, 0)\n",
    "    p_word_spam = (spam_count + 1) / (total_spam_words + vocab_size)\n",
    "    \n",
    "    ham_count = ham_word_count.get(word, 0)\n",
    "    p_word_ham = (ham_count + 1) / (total_ham_words + vocab_size)\n",
    "    \n",
    "    log_prob_spam += np.log(p_word_spam)\n",
    "    log_prob_ham += np.log(p_word_ham)\n",
    "    \n",
    "    print(f\"  {word}: P(word|Spam)={p_word_spam:.6f}, P(word|Ham)={p_word_ham:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Final log P(Spam|email) = {log_prob_spam:.4f}\")\n",
    "print(f\"Final log P(Ham|email) = {log_prob_ham:.4f}\")\n",
    "\n",
    "if log_prob_spam > log_prob_ham:\n",
    "    classification = \"SPAM\"\n",
    "else:\n",
    "    classification = \"HAM\"\n",
    "\n",
    "print(f\"\\n>>> Classification: {classification} <<<\")\n",
    "print(\"=\"*70)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Email 3:\n",
      "Message: Quick cash opportunity\n",
      "\n",
      "======================================================================\n",
      "Extracted words: ['quick', 'cash', 'opportunity']\n",
      "======================================================================\n",
      "\n",
      "Starting with priors:\n",
      "log P(Spam) = -2.0094\n",
      "log P(Ham) = -0.1439\n",
      "\n",
      "Multiplying by word likelihoods:\n",
      "  quick: P(word|Spam)=0.000043, P(word|Ham)=0.000130\n",
      "  cash: P(word|Spam)=0.002715, P(word|Ham)=0.000188\n",
      "  opportunity: P(word|Spam)=0.000043, P(word|Ham)=0.000058\n",
      "\n",
      "======================================================================\n",
      "Final log P(Spam|email) = -28.0222\n",
      "Final log P(Ham|email) = -27.4355\n",
      "\n",
      ">>> Classification: HAM <<<\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 72
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
