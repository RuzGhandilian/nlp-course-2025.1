{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T09:07:57.009830Z",
     "start_time": "2025-10-03T09:07:55.649298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tabulate import tabulate\n",
    "import  re, string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math"
   ],
   "id": "89df81d0e58e6081",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-03T09:07:57.827322Z",
     "start_time": "2025-10-03T09:07:57.042403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df = pd.read_csv(\"spam.csv\", encoding='latin1')"
   ],
   "id": "2d25f8b6bc22900a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\srbuh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T09:07:57.931750Z",
     "start_time": "2025-10-03T09:07:57.846084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = text.split()\n",
    "    text = re.findall(r\"[a-z]+\", text)\n",
    "    tokens = [word for word in tokens if word not in stop_words ]\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['v2'].apply(preprocess_text)"
   ],
   "id": "f52b895ca25b5a9d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T09:07:58.071453Z",
     "start_time": "2025-10-03T09:07:58.056686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_vocab(token_lists, min_freq = 1):\n",
    "    vocab = {}\n",
    "    word_freq = {}\n",
    "    for tokens in token_lists:\n",
    "        for token in tokens:\n",
    "            word_freq[token] = word_freq.get(token, 0) + 1\n",
    "    idx = 0\n",
    "    for word, freq in word_freq.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(df['tokens'])\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ],
   "id": "5369287ae2267783",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 9431\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T09:07:58.332949Z",
     "start_time": "2025-10-03T09:07:58.086411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def doc_to_bow(tokens, vocab):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            vec[vocab[token]] += 1\n",
    "    return vec\n",
    "\n",
    "df['bow_vector'] = df['tokens'].apply(lambda x: doc_to_bow(x, vocab))\n",
    "#print(df[['v1','v2','bow_vector']].head())"
   ],
   "id": "253708bab9bd48a7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T09:07:58.376504Z",
     "start_time": "2025-10-03T09:07:58.344381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spam_df = df[df['v1']=='spam']\n",
    "ham_df = df[df['v1']=='ham']\n",
    "\n",
    "# posterior probability\n",
    "P_spam = len(spam_df)/len(df)\n",
    "P_ham = len(ham_df)/len(df)\n",
    "\n",
    "spam_tokens_lists = spam_df['tokens'].tolist()\n",
    "ham_tokens_lists = ham_df['tokens'].tolist()\n",
    "\n",
    "spam_counts = np.zeros(len(vocab))\n",
    "ham_counts = np.zeros(len(vocab))\n",
    "\n",
    "for tokens in spam_tokens_lists:\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            spam_counts[vocab[token]] += 1\n",
    "\n",
    "for tokens in ham_tokens_lists:\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            ham_counts[vocab[token]] += 1\n",
    "\n",
    "spam = spam_counts.sum() #total\n",
    "ham = ham_counts.sum()\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "def p_word_given_class(index, counts, total, V):      # Laplace smoothing\n",
    "    return (counts[index] + 1) / (total + V)"
   ],
   "id": "5acf7a4d57b056fe",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T09:07:58.386622Z",
     "start_time": "2025-10-03T09:07:58.383390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def classify(tokens):\n",
    "    log_spam = math.log(P_spam)\n",
    "    log_ham = math.log(P_ham)\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            idx = vocab[token]\n",
    "            log_spam += math.log(p_word_given_class(idx, spam_counts, spam, vocab_len))\n",
    "            log_ham += math.log(p_word_given_class(idx, ham_counts, ham, vocab_len))\n",
    "\n",
    "    return \"spam\" if log_spam > log_ham else \"ham\""
   ],
   "id": "2b8086ffe96fa635",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T09:07:58.401689Z",
     "start_time": "2025-10-03T09:07:58.395629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spam_emails = [\n",
    "    \"Congratulations! You have won a free iPhone!\",\n",
    "    \"Get rich quick with this one simple trick!\",\n",
    "    \"Claim your free gift card now!\",\n",
    "    \"Limited time offer, buy now and save 50%\",\n",
    "    \"You have been selected for a cash reward\"\n",
    "]\n",
    "\n",
    "ham_emails = [\n",
    "    \"Hey, are we meeting for coffee tomorrow?\",\n",
    "    \"Please find attached the report for our meeting\",\n",
    "    \"Can you send me the notes from class?\",\n",
    "    \"Let's have lunch next week\",\n",
    "    \"Don't forget about the project deadline on Friday\"\n",
    "]\n",
    "\n",
    "for email in spam_emails + ham_emails:\n",
    "    tokens = preprocess_text(email)\n",
    "    prediction = classify(tokens)\n",
    "    print(f\"Email: {email}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(\".\"*50)"
   ],
   "id": "d9e2eea6a0dc4205",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email: Congratulations! You have won a free iPhone!\n",
      "Prediction: spam\n",
      "..................................................\n",
      "Email: Get rich quick with this one simple trick!\n",
      "Prediction: ham\n",
      "..................................................\n",
      "Email: Claim your free gift card now!\n",
      "Prediction: spam\n",
      "..................................................\n",
      "Email: Limited time offer, buy now and save 50%\n",
      "Prediction: ham\n",
      "..................................................\n",
      "Email: You have been selected for a cash reward\n",
      "Prediction: spam\n",
      "..................................................\n",
      "Email: Hey, are we meeting for coffee tomorrow?\n",
      "Prediction: ham\n",
      "..................................................\n",
      "Email: Please find attached the report for our meeting\n",
      "Prediction: ham\n",
      "..................................................\n",
      "Email: Can you send me the notes from class?\n",
      "Prediction: ham\n",
      "..................................................\n",
      "Email: Let's have lunch next week\n",
      "Prediction: ham\n",
      "..................................................\n",
      "Email: Don't forget about the project deadline on Friday\n",
      "Prediction: ham\n",
      "..................................................\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
