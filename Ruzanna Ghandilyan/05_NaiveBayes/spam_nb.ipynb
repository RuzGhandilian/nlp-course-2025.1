{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ead1db",
   "metadata": {},
   "source": "# Bernoulli Naive Bayes for Spam Detection\n"
  },
  {
   "cell_type": "code",
   "id": "b3ffa55a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T15:55:45.765757Z",
     "start_time": "2025-10-03T15:55:27.672691Z"
    }
   },
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "0ec03c68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T15:55:45.968772Z",
     "start_time": "2025-10-03T15:55:45.800449Z"
    }
   },
   "source": [
    "# load dataset with a minimal, robust parser\n",
    "# rationale: the csv may not be utf-8 and may not be comma-delimited\n",
    "df = pd.read_csv(\"spam.csv\", sep=None, engine=\"python\", encoding=\"latin1\")\n",
    "\n",
    "# select label/text columns using common conventions; fall back to first/last\n",
    "label_col = \"label\" if \"label\" in df.columns else (\"v1\" if \"v1\" in df.columns else df.columns[0])\n",
    "text_col  = \"text\"  if \"text\"  in df.columns else (\"v2\" if \"v2\" in df.columns else df.columns[-1])\n",
    "df = df[[label_col, text_col]].rename(columns={label_col: \"label\", text_col: \"text\"}).dropna()\n",
    "\n",
    "# normalize labels to {0=ham, 1=spam}\n",
    "label_map = {\"ham\": 0, \"spam\": 1}\n",
    "df[\"label\"] = df[\"label\"].astype(str).str.lower().map(label_map).astype(int)\n",
    "\n",
    "# minimal dataset review\n",
    "print('shape:', df.shape)\n",
    "print('class counts:\\n', df['label'].value_counts().sort_index())\n",
    "df.head(3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5572, 2)\n",
      "class counts:\n",
      " label\n",
      "0    4825\n",
      "1     747\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   label                                               text\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "32b29b48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T15:55:46.533177Z",
     "start_time": "2025-10-03T15:55:46.517471Z"
    }
   },
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"text\"], df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "d8a612cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T15:55:46.711925Z",
     "start_time": "2025-10-03T15:55:46.579977Z"
    }
   },
   "source": [
    "# binary presence/absence features as required by Bernoulli NB\n",
    "# note: CountVectorizer(lowercase=True by default) builds the vocabulary on the training set only\n",
    "vect = CountVectorizer(binary=True)\n",
    "Xtr = vect.fit_transform(X_train)\n",
    "Xte = vect.transform(X_test)\n",
    "\n",
    "V = Xtr.shape[1]                       # vocabulary size\n",
    "classes = np.array([0, 1])             # 0=ham, 1=spam\n",
    "Nc = np.bincount(y_train, minlength=2)  # documents per class\n",
    "priors = Nc / Nc.sum()                 # P(c)\n",
    "print('vocab size:', V, '| priors:', priors)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 7701 | priors: [0.86582903 0.13417097]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "c1ef9ff0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T15:55:46.971243Z",
     "start_time": "2025-10-03T15:55:46.957939Z"
    }
   },
   "source": [
    "# estimate Bernoulli likelihoods with Laplace smoothing\n",
    "# P(w|c) = (N_cw + 1) / (N_c + 2), where N_cw counts documents in class c that contain word w\n",
    "# compute N_cw by summing binary feature presence within each class\n",
    "\n",
    "# masks for classes\n",
    "mask0 = (y_train.values == 0)\n",
    "mask1 = ~mask0\n",
    "\n",
    "# sum over rows per class (Xtr is binary; sums -> doc counts containing each term)\n",
    "N0w = Xtr[mask0].sum(axis=0)  # shape (1, V)\n",
    "N1w = Xtr[mask1].sum(axis=0)  # shape (1, V)\n",
    "\n",
    "# convert to flat arrays\n",
    "N0w = np.asarray(N0w).ravel()\n",
    "N1w = np.asarray(N1w).ravel()\n",
    "\n",
    "# apply Laplace smoothing denominators: N_c + 2\n",
    "P_w_c0 = (N0w + 1.0) / (Nc[0] + 2.0)   # shape (V,)\n",
    "P_w_c1 = (N1w + 1.0) / (Nc[1] + 2.0)\n",
    "\n",
    "# stack into a (2, V) matrix for vectorized scoring\n",
    "P_w_c = np.vstack([P_w_c0, P_w_c1])\n",
    "\n",
    "# to avoid log(0), clamp extremely small/large values\n",
    "eps = 1e-12\n",
    "P_w_c = np.clip(P_w_c, eps, 1 - eps)\n",
    "\n",
    "print('example P(w|ham), P(w|spam) for first 5 terms:\\n',\n",
    "      np.vstack([P_w_c[0,:5], P_w_c[1,:5]]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example P(w|ham), P(w|spam) for first 5 terms:\n",
      " [[0.000259   0.000259   0.000259   0.000259   0.000259  ]\n",
      " [0.01833333 0.035      0.00333333 0.00333333 0.00333333]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "c646c812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T15:55:47.592311Z",
     "start_time": "2025-10-03T15:55:47.579629Z"
    }
   },
   "source": [
    "# inference: log P(c | x) âˆ log P(c) + sum_w x_w log P(w|c) + sum_w (1-x_w) log (1 - P(w|c))\n",
    "# efficient computation: \n",
    "#   sum_w (1-x_w) log(1-p) = sum_w log(1-p) + sum_w x_w [ -log(1-p) ]\n",
    "#   => score = log P(c) + sum_w log(1-p) + X * log( p / (1-p) )\n",
    "\n",
    "log_priors = np.log(priors)                 # shape (2,)\n",
    "log_one_minus = np.log(1.0 - P_w_c)         # shape (2, V)\n",
    "log_ratio = np.log(P_w_c) - log_one_minus   # shape (2, V)\n",
    "base = log_priors + log_one_minus.sum(axis=1)  # per-class constant, shape (2,)\n",
    "\n",
    "# sparse (n, V) @ (V, 2) -> (n, 2)\n",
    "scores = Xte @ log_ratio.T                  # (n_test, 2)\n",
    "scores = scores + base                      # broadcast add per-class base\n",
    "y_pred = scores.argmax(axis=1)              # argmax over classes\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "91be1239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T15:55:47.698255Z",
     "start_time": "2025-10-03T15:55:47.665415Z"
    }
   },
   "source": [
    "# evaluation: accuracy + precision/recall/F1 and confusion matrix\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\", zero_division=0)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"accuracy: {acc:.4f} | precision: {prec:.4f} | recall: {rec:.4f} | f1: {f1:.4f}\")\n",
    "print(\"confusion matrix:\\n\", cm)\n",
    "print(\"\\nclassification report:\\n\", classification_report(y_test, y_pred, target_names=[\"ham\",\"spam\"], zero_division=0))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9758 | precision: 1.0000 | recall: 0.8188 | f1: 0.9004\n",
      "confusion matrix:\n",
      " [[966   0]\n",
      " [ 27 122]]\n",
      "\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.99       966\n",
      "        spam       1.00      0.82      0.90       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.91      0.94      1115\n",
      "weighted avg       0.98      0.98      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
